\documentclass{article}
\usepackage{amsmath, tikz, pdfpages, float, booktabs}
\begin{document}
\title{Synopsis of Papers read}
\author{Edward, Sagar, Kapil}
\maketitle

\begin{itemize}
    \item
          \cite{tiwari_using_2024}
          Smaller Models trained via KD rely more on spurious correlations than the teacher model. this leads to loss in
          worst group performance / group fairness metrics. The dedier to solve this. the claim is that this correlation
          are learnt in the early layers of the student network and a readouts from these help. the readouts from early layers
          are more confident for the problematic instances and hence weighing the kd loss uisng them would be helpful based on the confidence
          margin.
          \[ \mathcal{L}_{student} =
          \sum_{D_w}^{} (1- \lambda) \cdot l_{ce}  \\
          + \lambda \cdot \textbf{wt} \cdot l_{ke} \] 
          where \(\bf{wt} = \exp^{\beta.\bf{cm}.\alpha} \)  
          Evaluations done on 4 debiasing benchmarks: CelebsA: Blond/Non Blond, Waterbirds: Landbird / Waterbird, MultiNLI: Entails/neutral/contradict,
          CivilComments-WILDS: toxic/ or not. Baselines used- Train twice: reweighing the losses from first classifier, Group DRO, Reused teacher heads.

          Model architecture used RESNETS t-50,student-18, bert-T, distillbert student. 
    \item
          \cite{jazbec_early-exit_2024} Talks about prediction sets in Early exit network that are nested and we can have anytime guarantee on them.
          Villes Theorem gives us a way to give these confidence intervals if we have access to some martingales. 
          Ideal Scenario: We compute the Posterior for the given test point,label(caveat thats the one we are making predictions for)
          Proposition 1: This predictive likelihood ratio is martingale. Realisable relaxation: Instead of taking \(W_t|D_* \)  from posterior of dataset 
          after adding a new point, we take it to be same as prior i.e \(W_t|D \)  The miscoverage probabibliity can be bounded by some exponential term with KL of distribution, and new instance
          
          For regression: 
    \item
          \cite{hinton_distilling_2015} Having acces to a large model that generalises well, the genrelisation can be
          induced to a smaller model by distillation. Here in combination with the cross entropy loss we also minimise the
          kl divergence of last layer logits with the teacher model
          The intuition being that even the negative examples give info about dataset that is not captured by the cross entropy loss.
          To read: HMM, mariginalisation, Large scale distributed deep networks, mixture of experts.
\end{itemize}



 

\bibliographystyle{acm}
\bibliography{optML.bib}


\end{document}