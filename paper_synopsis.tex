\documentclass{article}
\usepackage{amsmath, tikz, pdfpages, float, booktabs}
\begin{document}
\title{Synopsis of Papers read}
\author{Edward, Sagar, Kapil}
\maketitle

\begin{itemize}
    \item
          \cite{tiwari_using_2024}
          Smaller Models trained via KD rely more on spurious correlations than the teacher model. this leads to loss in
          worst group performance / group fairness metrics. The dedier to solve this. the claim is that this correlation
          are learnt in the early layers of the student network and a readouts from these help. the readouts from early layers
          are more confident for the problematic instances and hence weighing the kd loss uisng them would be helpful based on the confidence
          margin.
          \[ \mathcal{L}_{student} =
          \sum_{D_w}^{} (1- \lambda) \cdot l_{ce}  \\
          + \lambda \cdot \textbf{wt} \cdot l_{ke} \] 
          where \(\bf{wt} = \exp^{\beta.\bf{cm}.\alpha} \)  
          Evaluations done on 4 debiasing benchmarks: CelebsA: Blond/Non Blond, Waterbirds: Landbird / Waterbird, MultiNLI: Entails/neutral/contradict,
          CivilComments-WILDS: toxic/ or not. Baselines used- Train twice: reweighing the losses from first classifier, Group DRO, Reused teacher heads.

          Model architecture used RESNETS t-50,student-18, bert-T, distillbert student. 
    \item
          \cite{jazbec_early-exit_2024} Talks about prediction sets in Early exit network that are nested and we can have anytime guarantee on them.
          Villes Theorem gives us a way to give these confidence intervals if we have access to some martingales. 
          Ideal Scenario: We compute the Posterior for the given test point,label(caveat thats the one we are making predictions for)
          Proposition 1: This predictive likelihood ratio is martingale. Realisable relaxation: Instead of taking \(W_t|D_* \)  from posterior of dataset 
          after adding a new point, we take it to be same as prior i.e \(W_t|D \)  The miscoverage probabibliity can be bounded by some exponential term with KL of distribution, and new instance
          For regression: closed form of the posterior
    \item
          \cite{hinton_distilling_2015} Having acces to a large model that generalises well, the genrelisation can be
          induced to a smaller model by distillation. Here in combination with the cross entropy loss we also minimise the
          kl divergence of last layer logits with the teacher model
          The intuition being that even the negative examples give info about dataset that is not captured by the cross entropy loss.
          To read: HMM, mariginalisation, Large scale distributed deep networks, mixture of experts.
\end{itemize}


\section{Experiment Setup:}
Fork from feature bias paper:
Structure: Download data in directory defined in data/data.py, main experiment defined in run_exp 
Data directory has some pytorch dataset definitions defined. models
 

dump
% Knowledge distillation, or to use a bigger machine learning model in order to train a smaller one, has several potential upsides.  The most obvious use case is that a smaller model requires less computation, and if most of the quality can be transferred to a smaller model, that might be good enough for the desired purpose. However, as neural networks become increasingly complex, there is a risk that they may begin to capture intricate sub-patterns that degrade rather than enhance performance. Introducing a distilled, simplified version of the model can therefore not only approximate the performance of the original but, in some cases, even surpass it.

% \subsubsection{Knowledge Distillation}
% In their seminal work, Hinton et al. introduced the concept of knowledge distillation as a method for transferring knowledge from a large, complex model (the “teacher”) to a smaller, more efficient model (the “student”) \cite{hinton2015distilling}. The core idea is that instead of training the student model solely on hard labels, it is trained to match the soft output probabilities (often called “soft targets”) produced by the teacher. These soft targets encode rich information about the relative probabilities of incorrect classes, reflecting the teacher’s learned generalizations and inter-class similarities. By learning from these more informative targets, the student model can generalize better than if it were trained on hard labels alone. This approach enables the student to replicate much of the teacher’s performance while being significantly smaller and faster to deploy.


\bibliographystyle{acm}
\bibliography{optML.bib}


\end{document}