
@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/XQHZSLYI/1503.html:text/html;PDF:/Users/sagar/Documents/Courses/CSE IITB/Optim in ML/Papers/2015_hinton_vinyals_distilling_the_knowledge.pdf:application/pdf},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://link.springer.com/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	language = {en},
	number = {6},
	urldate = {2025-03-19},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = jun,
	year = {2021},
	pages = {1789--1819},
}

@article{ge_low-resolution_2019,
	title = {Low-resolution {Face} {Recognition} in the {Wild} via {Selective} {Knowledge} {Distillation}},
	volume = {28},
	issn = {1057-7149, 1941-0042},
	url = {http://arxiv.org/abs/1811.09998},
	doi = {10.1109/TIP.2018.2883743},
	abstract = {Typically, the deployment of face recognition models in the wild needs to identify low-resolution faces with extremely low computational cost. To address this problem, a feasible solution is compressing a complex face model to achieve higher speed and lower memory at the cost of minimal performance drop. Inspired by that, this paper proposes a learning approach to recognize low-resolution faces via selective knowledge distillation. In this approach, a two-stream convolutional neural network (CNN) is first initialized to recognize high-resolution faces and resolution-degraded faces with a teacher stream and a student stream, respectively. The teacher stream is represented by a complex CNN for high-accuracy recognition, and the student stream is represented by a much simpler CNN for low-complexity recognition. To avoid significant performance drop at the student stream, we then selectively distil the most informative facial features from the teacher stream by solving a sparse graph optimization problem, which are then used to regularize the fine-tuning process of the student stream. In this way, the student stream is actually trained by simultaneously handling two tasks with limited computational resources: approximating the most informative facial cues via feature regression, and recovering the missing facial cues via low-resolution face classification. Experimental results show that the student stream performs impressively in recognizing low-resolution faces and costs only 0.15MB memory and runs at 418 faces per second on CPU and 9,433 faces per second on GPU.},
	number = {4},
	urldate = {2025-03-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Ge, Shiming and Zhao, Shengwei and Li, Chenyu and Li, Jia},
	month = apr,
	year = {2019},
	note = {arXiv:1811.09998 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2051--2062},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/G2PBHPMS/1811.html:text/html},
}

@article{wang_private_2018,
	title = {Private {Model} {Compression} via {Knowledge} {Distillation}},
	volume = {abs/1811.05072},
	url = {https://api.semanticscholar.org/CorpusID:53288870},
	journal = {ArXiv},
	author = {Wang, Ji and Bao, Weidong and Sun, Lichao and Zhu, Xiaomin and Cao, Bokai and Yu, Philip S.},
	year = {2018},
}

@misc{bagherinezhad_label_2018,
	title = {Label {Refinery}: {Improving} {ImageNet} {Classification} through {Label} {Progression}},
	shorttitle = {Label {Refinery}},
	url = {http://arxiv.org/abs/1805.02641},
	abstract = {Among the three main components (data, labels, and models) of any supervised learning system, data and models have been the main subjects of active research. However, studying labels and their properties has received very little attention. Current principles and paradigms of labeling impose several challenges to machine learning algorithms. Labels are often incomplete, ambiguous, and redundant. In this paper we study the effects of various properties of labels and introduce the Label Refinery: an iterative procedure that updates the ground truth labels after examining the entire dataset. We show significant gain using refined labels across a wide range of models. Using a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from 50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.},
	urldate = {2025-03-19},
	publisher = {arXiv},
	author = {Bagherinezhad, Hessam and Horton, Maxwell and Rastegari, Mohammad and Farhadi, Ali},
	month = may,
	year = {2018},
	note = {arXiv:1805.02641 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/A8WZA29J/1805.html:text/html},
}
@misc{hartford_spectrum_2024,
	title = {Spectrum: {Targeted} {Training} on {Signal} to {Noise} {Ratio}},
	shorttitle = {Spectrum},
	url = {http://arxiv.org/abs/2406.06623},
	abstract = {Efficiently post-training large language models remains a challenging task due to the vast computational resources required. We present Spectrum, a method that accelerates LLM training by selectively targeting layer modules based on their signal-to-noise ratio (SNR), and freezing the remaining modules. Our approach, which utilizes an algorithm to compute module SNRs prior to training, has shown to effectively match the performance of full fine-tuning while reducing GPU memory usage. Experiments comparing Spectrum to existing methods such as QLoRA demonstrate its effectiveness in terms of model quality and VRAM efficiency in distributed environments.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Hartford, Eric and Atkins, Lucas and Neto, Fernando Fernandes and Golchinfar, David},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06623 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/YSTUKB48/2406.html:text/html},
}

@inproceedings{tiwari_using_2024,
	address = {Waikoloa, HI, USA},
	title = {Using {Early} {Readouts} to {Mediate} {Featural} {Bias} in {Distillation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350318920},
	url = {https://ieeexplore.ieee.org/document/10484228/},
	doi = {10.1109/WACV57701.2024.00262},
	urldate = {2025-02-03},
	booktitle = {2024 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Tiwari, Rishabh and Sivasubramanian, Durga and Mekala, Anmol and Ramakrishnan, Ganesh and Shenoy, Pradeep},
	month = jan,
	year = {2024},
	pages = {2626--2635},
}

@misc{raman_online_2024,
	title = {Online {Classification} with {Predictions}},
	url = {http://arxiv.org/abs/2405.14066},
	abstract = {We study online classification when the learner has access to predictions about future examples. We design an online learner whose expected regret is never worse than the worst-case regret, gracefully improves with the quality of the predictions, and can be significantly better than the worst-case regret when the predictions of future examples are accurate. As a corollary, we show that if the learner is always guaranteed to observe data where future examples are easily predictable, then online learning can be as easy as transductive online learning. Our results complement recent work in online algorithms with predictions and smoothed online classification, which go beyond a worse-case analysis by using machine-learned predictions and distributional assumptions respectively.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Raman, Vinod and Tewari, Ambuj},
	month = may,
	year = {2024},
	note = {arXiv:2405.14066 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/ZMRIRNBC/2405.html:text/html},
}

@misc{jazbec_early-exit_2024,
	title = {Early-{Exit} {Neural} {Networks} with {Nested} {Prediction} {Sets}},
	url = {http://arxiv.org/abs/2311.05931},
	abstract = {Early-exit neural networks (EENNs) enable adaptive and efficient inference by providing predictions at multiple stages during the forward pass. In safety-critical applications, these predictions are meaningful only when accompanied by reliable uncertainty estimates. A popular method for quantifying the uncertainty of predictive models is the use of prediction sets. However, we demonstrate that standard techniques such as conformal prediction and Bayesian credible sets are not suitable for EENNs. They tend to generate non-nested sets across exits, meaning that labels deemed improbable at one exit may reappear in the prediction set of a subsequent exit. To address this issue, we investigate anytime-valid confidence sequences (AVCSs), an extension of traditional confidence intervals tailored for data-streaming scenarios. These sequences are inherently nested and thus well-suited for an EENN's sequential predictions. We explore the theoretical and practical challenges of using AVCSs in EENNs and show that they indeed yield nested sets across exits. Thus our work presents a promising approach towards fast, yet still safe, predictive modeling},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Jazbec, Metod and Forré, Patrick and Mandt, Stephan and Zhang, Dan and Nalisnick, Eric},
	month = jun,
	year = {2024},
	note = {arXiv:2311.05931 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/SALMYSME/2311.html:text/html},
}

@misc{wadia_gentle_2024,
	title = {A {Gentle} {Introduction} to {Gradient}-{Based} {Optimization} and {Variational} {Inequalities} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2309.04877},
	abstract = {The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.},
	urldate = {2025-03-31},
	publisher = {arXiv},
	author = {Wadia, Neha S. and Dandi, Yatin and Jordan, Michael I.},
	month = feb,
	year = {2024},
	note = {arXiv:2309.04877 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/X54Q964B/2309.html:text/html},
}

@misc{berta_rethinking_2025,
	title = {Rethinking {Early} {Stopping}: {Refine}, {Then} {Calibrate}},
	shorttitle = {Rethinking {Early} {Stopping}},
	url = {http://arxiv.org/abs/2501.19195},
	abstract = {Machine learning classifiers often produce probabilistic predictions that are critical for accurate and interpretable decision-making in various domains. The quality of these predictions is generally evaluated with proper losses like cross-entropy, which decompose into two components: calibration error assesses general under/overconfidence, while refinement error measures the ability to distinguish different classes. In this paper, we provide theoretical and empirical evidence that these two errors are not minimized simultaneously during training. Selecting the best training epoch based on validation loss thus leads to a compromise point that is suboptimal for both calibration error and, most importantly, refinement error. To address this, we introduce a new metric for early stopping and hyperparameter tuning that makes it possible to minimize refinement error during training. The calibration error is minimized after training, using standard techniques. Our method integrates seamlessly with any architecture and consistently improves performance across diverse classification tasks.},
	urldate = {2025-03-31},
	publisher = {arXiv},
	author = {Berta, Eugène and Holzmüller, David and Jordan, Michael I. and Bach, Francis},
	month = jan,
	year = {2025},
	note = {arXiv:2501.19195 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/sagar/Zotero/storage/P7EXMP8T/2501.html:text/html},
}

