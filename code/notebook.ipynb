{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing dedier framework for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.39 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets>=2.19 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (3.4.0)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: torch in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (2022.3.15)\n",
      "Requirement already satisfied: requests in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.39) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (1.4.1)\n",
      "Requirement already satisfied: xxhash in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.19) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.19) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: psutil in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.19) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers>=4.39) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers>=4.39) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers>=4.39) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=2.19) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=2.19) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/edvinfasth/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.19) (1.16.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate, evaluate\n",
      "Successfully installed accelerate-1.6.0 evaluate-0.4.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers>=4.39\" \"datasets>=2.19\" \"evaluate\" torch accelerate\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_all = load_dataset(\"google/civil_comments\", split=\"train\")  # 2 M rows\n",
    "# use the built‑in official train/val/test indices released by Jigsaw\n",
    "splits = DatasetDict({\n",
    "    \"train\": ds_all.filter(lambda ex: ex[\"split\"] == \"train\"),\n",
    "    \"validation\": ds_all.filter(lambda ex: ex[\"split\"] == \"val\"),\n",
    "    \"test\": ds_all.filter(lambda ex: ex[\"split\"] == \"test\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tok_t = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "def tokenize(ex):\n",
    "    return tok_t(\n",
    "        ex[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "splits_tok = splits.map(tokenize, batched=True).rename_column(\"toxicity\", \"labels\")\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-large-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"teacher_out\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e‑5,      # authors’ best for Civil Comments  [oai_citation_attribution:0‡arXiv](https://arxiv.org/pdf/2310.18590)\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_t = Trainer(\n",
    "    model=teacher,\n",
    "    args=args,\n",
    "    train_dataset=splits_tok[\"train\"],\n",
    "    eval_dataset=splits_tok[\"validation\"],\n",
    ")\n",
    "trainer_t.train()\n",
    "teacher.save_pretrained(\"teacher_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "class DEDIERStudent(nn.Module):\n",
    "    def __init__(self, base_ckpt=\"distilbert-base-uncased\", num_labels=2, aux_layer=1):\n",
    "        super().__init__()\n",
    "        cfg = AutoConfig.from_pretrained(base_ckpt, output_hidden_states=True)\n",
    "        self.encoder = AutoModel.from_pretrained(base_ckpt, config=cfg)\n",
    "        self.classifier = nn.Linear(cfg.hidden_size, num_labels)\n",
    "        # early‑readout head (two‑layer MLP works best  [oai_citation_attribution:1‡arXiv](https://arxiv.org/pdf/2310.18590))\n",
    "        self.aux_layer = aux_layer          # 1 = after first transformer block\n",
    "        self.aux_head = nn.Sequential(\n",
    "            nn.Linear(cfg.hidden_size, cfg.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(cfg.hidden_size, num_labels),\n",
    "        )\n",
    "        self.temperature = 2.0              # KD temperature\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, teacher_logits=None):\n",
    "        out = self.encoder(\n",
    "            input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        )\n",
    "        logits = self.classifier(out.last_hidden_state[:, 0])  # [CLS]\n",
    "        # early readout\n",
    "        h_aux = out.hidden_states[self.aux_layer][:, 0]        # layer‑k [CLS]\n",
    "        logits_aux = self.aux_head(h_aux)\n",
    "\n",
    "        if labels is None:\n",
    "            return logits, logits_aux     # inference\n",
    "\n",
    "        # --- DEDIER losses ---\n",
    "        ce = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        # knowledge‑distillation (teacher => student)\n",
    "        kd = nn.functional.kl_div(\n",
    "            nn.functional.log_softmax(logits / self.temperature, dim=-1),\n",
    "            nn.functional.softmax(teacher_logits / self.temperature, dim=-1),\n",
    "            reduction=\"batchmean\",\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # error flag: confidently wrong early readout?\n",
    "        probs_aux = nn.functional.softmax(logits_aux, dim=-1)\n",
    "        top2 = probs_aux.topk(2, dim=-1).values\n",
    "        margin = (top2[:, 0] - top2[:, 1]).detach()\n",
    "        wrong = (logits_aux.argmax(-1) != labels).float()\n",
    "        w_i = (1 + margin ** 3) * wrong          # β = 3 from paper\n",
    "        loss_dedier = (w_i * kd).mean()          # α implicit inside w_i\n",
    "\n",
    "        return ce + 0.05 * loss_dedier, logits   # α = 0.05  [oai_citation_attribution:2‡arXiv](https://arxiv.org/pdf/2310.18590)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_s = tok_t               # same vocab\n",
    "\n",
    "def collate(batch):\n",
    "    # fetch teacher logits for KD\n",
    "    with torch.no_grad():\n",
    "        t_out = teacher(\n",
    "            torch.tensor([item[\"input_ids\"] for item in batch]),\n",
    "            attention_mask=torch.tensor([item[\"attention_mask\"] for item in batch]),\n",
    "        ).logits\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch]),\n",
    "        \"labels\": labels,\n",
    "        \"teacher_logits\": t_out,\n",
    "    }\n",
    "\n",
    "student = DEDIERStudent()\n",
    "\n",
    "args_s = TrainingArguments(\n",
    "    output_dir=\"student_out\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e‑5,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer_s = Trainer(\n",
    "    model=student,\n",
    "    args=args_s,\n",
    "    train_dataset=splits_tok[\"train\"],\n",
    "    eval_dataset=splits_tok[\"validation\"],\n",
    "    data_collator=collate,\n",
    ")\n",
    "trainer_s.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "roc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "id_cols = [c for c in splits[\"test\"].column_names if c.endswith(\"_identity\")]\n",
    "\n",
    "def predict(ds, model):\n",
    "    logits = []\n",
    "    for chunk in ds.map(tokenize, batched=True).iter(batch_size=128):\n",
    "        with torch.no_grad():\n",
    "            l, _ = model(\n",
    "                torch.tensor(chunk[\"input_ids\"]),\n",
    "                attention_mask=torch.tensor(chunk[\"attention_mask\"]),\n",
    "            )\n",
    "        logits.append(l.softmax(-1)[:, 1].cpu())\n",
    "    return torch.cat(logits)\n",
    "\n",
    "y_hat = predict(splits[\"test\"], student)\n",
    "y_true = splits[\"test\"][\"toxicity\"] >= 0.5\n",
    "\n",
    "# overall AUROC\n",
    "print(\"overall:\", roc.compute(prediction_scores=y_hat, references=y_true)[\"roc_auc\"])\n",
    "\n",
    "# subgroup AUROC\n",
    "for g in id_cols:\n",
    "    mask = splits[\"test\"][g] == 1\n",
    "    if mask.sum() > 0:\n",
    "        print(g, roc.compute(prediction_scores=y_hat[mask], references=y_true[mask])[\"roc_auc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
